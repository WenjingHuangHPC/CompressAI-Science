{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a2caa31",
   "metadata": {},
   "source": [
    "# Step 0 — Specify config  and build TensorRT engines\n",
    "\n",
    "Before running CAICE inference, please **choose a component-level precision** and **build TensorRT engines** from your FP32 ONNX.\n",
    "### 1) Specify component precision \n",
    "\n",
    "Use the format:\n",
    "\n",
    "* `ga-fp8`, `ga-fp16`\n",
    "* `gs-fp8`, `gs-fp16`\n",
    "* `ha-fp8`, `ha-fp16`\n",
    "* `hs-fp8`, `hs-fp16`\n",
    "\n",
    "default configs:\n",
    "\n",
    "* `ga-fp8,gs-fp16,ha-fp8(if exist),hs-fp8(if exist)`\n",
    "\n",
    "> Note: For FP8, the build script will quantize the corresponding sub-ONNX (Q/DQ) and then build a **strongly-typed** TensorRT engine.\n",
    "\n",
    "### 2) Define component boundaries (`config.json`)\n",
    "\n",
    "After choosing the precision, you must define **graph boundaries** for each component in `boundaries.config`.\n",
    "The build script uses these boundaries to **extract sub-graphs** (g_a / g_s / h_a / h_s) from the full FP32 ONNX before quantizing and building engines.\n",
    "\n",
    "### What to provide for each component\n",
    "\n",
    "For every component, specify:\n",
    "\n",
    "* `inputs`: a list of **input tensor names** in the ONNX graph\n",
    "* `outputs`: a list of **output tensor names** in the ONNX graph\n",
    "\n",
    "Example (JSON):\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"ga\": { \"inputs\": [\"input\"], \"outputs\": [\"/g_a/g_a.6/Conv_output_0\"] },\n",
    "  \"ha\": { \"inputs\": [\"/g_a/g_a.6/Conv_output_0\"], \"outputs\": [\"<ha_out_tensor>\"] },\n",
    "  \"hs\": { \"inputs\": [\"<hs_in_tensor>\"], \"outputs\": [\"/entropy_bottleneck/Transpose_1_output_0\"] },\n",
    "  \"gs\": { \"inputs\": [\"/entropy_bottleneck/Transpose_1_output_0\"], \"outputs\": [\"output\"] }\n",
    "}\n",
    "```\n",
    "\n",
    "### Tips\n",
    "\n",
    "* Tensor names must match **exactly** what appears in the exported ONNX (case-sensitive).\n",
    "* You can inspect tensor names using **Netron** or by printing ONNX graph I/O names in Python.\n",
    "* If you only plan to accelerate a subset of components, you can still define all boundaries now and only build engines for the components listed in your precision config.\n",
    "\n",
    "\n",
    "### 3) Prepare calibration data and input shapes\n",
    "\n",
    "For **FP8 components**, calibration data is required to determine quantization scales.\n",
    "\n",
    "You must also specify the **exact input shape** used to build engines, since TensorRT engines are shape-specific.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0da4239f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_233078/2810950779.py:19: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(\n",
      "/tmp/ipykernel_233078/2810950779.py:34: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import onnx\n",
    "from compressai.zoo import bmshj2018_factorized\n",
    "\n",
    "quality = 1\n",
    "project_dir = \"/hwj\"\n",
    "device = \"cuda:0\"\n",
    "\n",
    "# load model\n",
    "model = bmshj2018_factorized(quality=quality, pretrained=False)\n",
    "state = torch.load(f\"{project_dir}/data/model/bmshj2018-factorized-prior-{quality}.pth\", map_location=device)\n",
    "model.load_state_dict(state)\n",
    "model.eval().to(device)\n",
    "\n",
    "# to fp32 onnx\n",
    "dummy_input = torch.randn((512, 3, 128, 128), device=device, dtype=torch.float32)\n",
    "onnx_path = f\"{project_dir}/data/model/onnx/bmshj2018-factorized-prior-{quality}-f32.onnx\"\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    onnx_path,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    opset_version=17,\n",
    ")\n",
    "\n",
    "# to fp16 onnx\n",
    "model.eval().to(device).to(torch.float16)\n",
    "\n",
    "dummy_input = torch.randn((512, 3, 128, 128), device=device, dtype=torch.float16)\n",
    "onnx_path = f\"{project_dir}/data/model/onnx/bmshj2018-factorized-prior-{quality}-f16.onnx\"\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    onnx_path,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    opset_version=17,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3478b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Extracted ga (fp16): /hwj/project/CompressAI-Science/examples/out_engines/subonnx/bmshj2018-factorized-q1/ga_fp16.onnx\n",
      "[Skip] boundaries missing component: ha\n",
      "[Skip] boundaries missing component: hs\n",
      "[OK] Extracted gs (fp16): /hwj/project/CompressAI-Science/examples/out_engines/subonnx/bmshj2018-factorized-q1/gs_fp16.onnx\n",
      "\n",
      "[Engine] Building ga engine (fp16): /hwj/project/CompressAI-Science/examples/out_engines/engines/bmshj2018-factorized-q1/ga/fp16.engine\n",
      "trtexec --onnx=/hwj/project/CompressAI-Science/examples/out_engines/subonnx/bmshj2018-factorized-q1/ga_fp16.onnx --saveEngine=/hwj/project/CompressAI-Science/examples/out_engines/engines/bmshj2018-factorized-q1/ga/fp16.engine --fp16\n",
      "&&&& RUNNING TensorRT.trtexec [TensorRT v101200] [b36] # trtexec --onnx=/hwj/project/CompressAI-Science/examples/out_engines/subonnx/bmshj2018-factorized-q1/ga_fp16.onnx --saveEngine=/hwj/project/CompressAI-Science/examples/out_engines/engines/bmshj2018-factorized-q1/ga/fp16.engine --fp16\n",
      "[01/12/2026-14:32:15] [I] === Model Options ===\n",
      "[01/12/2026-14:32:15] [I] Format: ONNX\n",
      "[01/12/2026-14:32:15] [I] Model: /hwj/project/CompressAI-Science/examples/out_engines/subonnx/bmshj2018-factorized-q1/ga_fp16.onnx\n",
      "[01/12/2026-14:32:15] [I] Output:\n",
      "[01/12/2026-14:32:15] [I] === Build Options ===\n",
      "[01/12/2026-14:32:15] [I] Memory Pools: workspace: default, dlaSRAM: default, dlaLocalDRAM: default, dlaGlobalDRAM: default, tacticSharedMem: default\n",
      "[01/12/2026-14:32:15] [I] avgTiming: 8\n",
      "[01/12/2026-14:32:15] [I] Precision: FP32+FP16\n",
      "[01/12/2026-14:32:15] [I] LayerPrecisions: \n",
      "[01/12/2026-14:32:15] [I] Layer Device Types: \n",
      "[01/12/2026-14:32:15] [I] Calibration: \n",
      "[01/12/2026-14:32:15] [I] Refit: Disabled\n",
      "[01/12/2026-14:32:15] [I] Strip weights: Disabled\n",
      "[01/12/2026-14:32:15] [I] Version Compatible: Disabled\n",
      "[01/12/2026-14:32:15] [I] ONNX Plugin InstanceNorm: Disabled\n",
      "[01/12/2026-14:32:15] [I] ONNX kENABLE_UINT8_AND_ASYMMETRIC_QUANTIZATION_DLA flag: Disabled\n",
      "[01/12/2026-14:32:15] [I] TensorRT runtime: full\n",
      "[01/12/2026-14:32:15] [I] Lean DLL Path: \n",
      "[01/12/2026-14:32:15] [I] Tempfile Controls: { in_memory: allow, temporary: allow }\n",
      "[01/12/2026-14:32:15] [I] Exclude Lean Runtime: Disabled\n",
      "[01/12/2026-14:32:15] [I] Sparsity: Disabled\n",
      "[01/12/2026-14:32:15] [I] Safe mode: Disabled\n",
      "[01/12/2026-14:32:15] [I] Build DLA standalone loadable: Disabled\n",
      "[01/12/2026-14:32:15] [I] Allow GPU fallback for DLA: Disabled\n",
      "[01/12/2026-14:32:15] [I] DirectIO mode: Disabled\n",
      "[01/12/2026-14:32:15] [I] Restricted mode: Disabled\n",
      "[01/12/2026-14:32:15] [I] Skip inference: Disabled\n",
      "[01/12/2026-14:32:15] [I] Save engine: /hwj/project/CompressAI-Science/examples/out_engines/engines/bmshj2018-factorized-q1/ga/fp16.engine\n",
      "[01/12/2026-14:32:15] [I] Load engine: \n",
      "[01/12/2026-14:32:15] [I] Profiling verbosity: 0\n",
      "[01/12/2026-14:32:15] [I] Tactic sources: Using default tactic sources\n",
      "[01/12/2026-14:32:15] [I] timingCacheMode: local\n",
      "[01/12/2026-14:32:15] [I] timingCacheFile: \n",
      "[01/12/2026-14:32:15] [I] Enable Compilation Cache: Enabled\n",
      "[01/12/2026-14:32:15] [I] Enable Monitor Memory: Disabled\n",
      "[01/12/2026-14:32:15] [I] errorOnTimingCacheMiss: Disabled\n",
      "[01/12/2026-14:32:15] [I] Preview Features: Use default preview flags.\n",
      "[01/12/2026-14:32:15] [I] MaxAuxStreams: -1\n",
      "[01/12/2026-14:32:15] [I] BuilderOptimizationLevel: -1\n",
      "[01/12/2026-14:32:15] [I] MaxTactics: -1\n",
      "[01/12/2026-14:32:15] [I] Calibration Profile Index: 0\n",
      "[01/12/2026-14:32:15] [I] Weight Streaming: Disabled\n",
      "[01/12/2026-14:32:15] [I] Runtime Platform: Same As Build\n",
      "[01/12/2026-14:32:15] [I] Debug Tensors: \n",
      "[01/12/2026-14:32:15] [I] Distributive Independence: Disabled\n",
      "[01/12/2026-14:32:15] [I] Mark Unfused Tensors As Debug Tensors: Disabled\n",
      "[01/12/2026-14:32:15] [I] Input(s)s format: fp32:CHW\n",
      "[01/12/2026-14:32:15] [I] Output(s)s format: fp32:CHW\n",
      "[01/12/2026-14:32:15] [I] Input build shapes: model\n",
      "[01/12/2026-14:32:15] [I] Input calibration shapes: model\n",
      "[01/12/2026-14:32:15] [I] === System Options ===\n",
      "[01/12/2026-14:32:15] [I] Device: 0\n",
      "[01/12/2026-14:32:15] [I] DLACore: \n",
      "[01/12/2026-14:32:15] [I] Plugins:\n",
      "[01/12/2026-14:32:15] [I] setPluginsToSerialize:\n",
      "[01/12/2026-14:32:15] [I] dynamicPlugins:\n",
      "[01/12/2026-14:32:15] [I] ignoreParsedPluginLibs: 0\n",
      "[01/12/2026-14:32:15] [I] \n",
      "[01/12/2026-14:32:15] [I] === Inference Options ===\n",
      "[01/12/2026-14:32:15] [I] Batch: Explicit\n",
      "[01/12/2026-14:32:15] [I] Input inference shapes: model\n",
      "[01/12/2026-14:32:15] [I] Iterations: 10\n",
      "[01/12/2026-14:32:15] [I] Duration: 3s (+ 200ms warm up)\n",
      "[01/12/2026-14:32:15] [I] Sleep time: 0ms\n",
      "[01/12/2026-14:32:15] [I] Idle time: 0ms\n",
      "[01/12/2026-14:32:15] [I] Inference Streams: 1\n",
      "[01/12/2026-14:32:15] [I] ExposeDMA: Disabled\n",
      "[01/12/2026-14:32:15] [I] Data transfers: Enabled\n",
      "[01/12/2026-14:32:15] [I] Spin-wait: Disabled\n",
      "[01/12/2026-14:32:15] [I] Multithreading: Disabled\n",
      "[01/12/2026-14:32:15] [I] CUDA Graph: Disabled\n",
      "[01/12/2026-14:32:15] [I] Separate profiling: Disabled\n",
      "[01/12/2026-14:32:15] [I] Time Deserialize: Disabled\n",
      "[01/12/2026-14:32:15] [I] Time Refit: Disabled\n",
      "[01/12/2026-14:32:15] [I] NVTX verbosity: 0\n",
      "[01/12/2026-14:32:15] [I] Persistent Cache Ratio: 0\n",
      "[01/12/2026-14:32:15] [I] Optimization Profile Index: 0\n",
      "[01/12/2026-14:32:15] [I] Weight Streaming Budget: 100.000000%\n",
      "[01/12/2026-14:32:15] [I] Inputs:\n",
      "[01/12/2026-14:32:15] [I] Debug Tensor Save Destinations:\n",
      "[01/12/2026-14:32:15] [I] Dump All Debug Tensor in Formats: \n",
      "[01/12/2026-14:32:15] [I] === Reporting Options ===\n",
      "[01/12/2026-14:32:15] [I] Verbose: Disabled\n",
      "[01/12/2026-14:32:15] [I] Averages: 10 inferences\n",
      "[01/12/2026-14:32:15] [I] Percentiles: 90,95,99\n",
      "[01/12/2026-14:32:15] [I] Dump refittable layers:Disabled\n",
      "[01/12/2026-14:32:15] [I] Dump output: Disabled\n",
      "[01/12/2026-14:32:15] [I] Profile: Disabled\n",
      "[01/12/2026-14:32:15] [I] Export timing to JSON file: \n",
      "[01/12/2026-14:32:15] [I] Export output to JSON file: \n",
      "[01/12/2026-14:32:15] [I] Export profile to JSON file: \n",
      "[01/12/2026-14:32:15] [I] \n",
      "[01/12/2026-14:32:15] [I] === Device Information ===\n",
      "[01/12/2026-14:32:15] [I] Available Devices: \n",
      "[01/12/2026-14:32:15] [I]   Device 0: \"NVIDIA H100 PCIe\" UUID: GPU-3e2fdfe4-208f-4000-27c1-3ecf36260172\n",
      "[01/12/2026-14:32:15] [I]   Device 1: \"NVIDIA H100 PCIe\" UUID: GPU-ad162803-098e-b980-5e1b-5623d5adc58b\n",
      "[01/12/2026-14:32:15] [I] Selected Device: NVIDIA H100 PCIe\n",
      "[01/12/2026-14:32:15] [I] Selected Device ID: 0\n",
      "[01/12/2026-14:32:15] [I] Selected Device UUID: GPU-3e2fdfe4-208f-4000-27c1-3ecf36260172\n",
      "[01/12/2026-14:32:15] [I] Compute Capability: 9.0\n",
      "[01/12/2026-14:32:15] [I] SMs: 114\n",
      "[01/12/2026-14:32:15] [I] Device Global Memory: 81079 MiB\n",
      "[01/12/2026-14:32:15] [I] Shared Memory per SM: 228 KiB\n",
      "[01/12/2026-14:32:15] [I] Memory Bus Width: 5120 bits (ECC enabled)\n",
      "[01/12/2026-14:32:15] [I] Application Compute Clock Rate: 1.755 GHz\n",
      "[01/12/2026-14:32:15] [I] Application Memory Clock Rate: 1.593 GHz\n",
      "[01/12/2026-14:32:15] [I] \n",
      "[01/12/2026-14:32:15] [I] Note: The application clock rates do not reflect the actual clock rates that the GPU is currently running at.\n",
      "[01/12/2026-14:32:15] [I] \n",
      "[01/12/2026-14:32:15] [I] TensorRT version: 10.12.0\n",
      "[01/12/2026-14:32:15] [I] Loading standard plugins\n",
      "[01/12/2026-14:32:15] [I] [TRT] [MemUsageChange] Init CUDA: CPU +2, GPU +0, now: CPU 44, GPU 5903 (MiB)\n",
      "[01/12/2026-14:32:18] [I] [TRT] [MemUsageChange] Init builder kernel library: CPU +1927, GPU +8, now: CPU 2173, GPU 5913 (MiB)\n",
      "[01/12/2026-14:32:18] [I] Start parsing network model.\n",
      "[01/12/2026-14:32:18] [I] [TRT] ----------------------------------------------------------------\n",
      "[01/12/2026-14:32:18] [I] [TRT] Input filename:   /hwj/project/CompressAI-Science/examples/out_engines/subonnx/bmshj2018-factorized-q1/ga_fp16.onnx\n",
      "[01/12/2026-14:32:18] [I] [TRT] ONNX IR version:  0.0.8\n",
      "[01/12/2026-14:32:18] [I] [TRT] Opset version:    17\n",
      "[01/12/2026-14:32:18] [I] [TRT] Producer name:    onnx.utils.extract_model\n",
      "[01/12/2026-14:32:18] [I] [TRT] Producer version: \n",
      "[01/12/2026-14:32:18] [I] [TRT] Domain:           \n",
      "[01/12/2026-14:32:18] [I] [TRT] Model version:    0\n",
      "[01/12/2026-14:32:18] [I] [TRT] Doc string:       \n",
      "[01/12/2026-14:32:18] [I] [TRT] ----------------------------------------------------------------\n",
      "[01/12/2026-14:32:18] [I] Finished parsing network model. Parse time: 0.00613437\n",
      "[01/12/2026-14:32:18] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[01/12/2026-14:33:39] [I] [TRT] Compiler backend is used during engine build.\n",
      "[01/12/2026-14:33:52] [I] [TRT] Detected 1 inputs and 1 output network tensors.\n",
      "[01/12/2026-14:33:53] [I] [TRT] Total Host Persistent Memory: 55120 bytes\n",
      "[01/12/2026-14:33:53] [I] [TRT] Total Device Persistent Memory: 0 bytes\n",
      "[01/12/2026-14:33:53] [I] [TRT] Max Scratch Memory: 33280 bytes\n",
      "[01/12/2026-14:33:53] [I] [TRT] [BlockAssignment] Started assigning block shifts. This will take 48 steps to complete.\n",
      "[01/12/2026-14:33:53] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 1.99143ms to assign 18 blocks to 48 nodes requiring 1610749952 bytes.\n",
      "[01/12/2026-14:33:53] [I] [TRT] Total Activation Memory: 1610747392 bytes\n",
      "[01/12/2026-14:33:53] [I] [TRT] Total Weights Memory: 2997504 bytes\n",
      "[01/12/2026-14:33:53] [I] [TRT] Compiler backend is used during engine execution.\n",
      "[01/12/2026-14:33:53] [I] [TRT] Engine generation completed in 95.1904 seconds.\n",
      "[01/12/2026-14:33:53] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 1 MiB, GPU 3072 MiB\n",
      "[01/12/2026-14:33:53] [I] Engine built in 95.2441 sec.\n",
      "[01/12/2026-14:33:53] [I] Created engine with size: 3.41812 MiB\n",
      "[01/12/2026-14:33:53] [I] [TRT] Loaded engine size: 3 MiB\n",
      "[01/12/2026-14:33:53] [I] Engine deserialized in 0.0138792 sec.\n",
      "[01/12/2026-14:33:53] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +1536, now: CPU 0, GPU 1538 (MiB)\n",
      "[01/12/2026-14:33:53] [I] Setting persistentCacheLimit to 0 bytes.\n",
      "[01/12/2026-14:33:53] [I] Created execution context with device memory size: 1536.13 MiB\n",
      "[01/12/2026-14:33:53] [I] Using random values for input input\n",
      "[01/12/2026-14:33:54] [I] Input binding for input with dimensions 512x3x128x128 is created.\n",
      "[01/12/2026-14:33:54] [I] Output binding for /g_a/g_a.6/Conv_output_0 with dimensions 512x192x8x8 is created.\n",
      "[01/12/2026-14:33:54] [I] Starting inference\n",
      "[01/12/2026-14:33:57] [I] Warmup completed 41 queries over 200 ms\n",
      "[01/12/2026-14:33:57] [I] Timing trace has 582 queries over 3.01303 s\n",
      "[01/12/2026-14:33:57] [I] \n",
      "[01/12/2026-14:33:57] [I] === Trace details ===\n",
      "[01/12/2026-14:33:57] [I] Trace averages of 10 runs:\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.06996 ms - Host latency: 6.22533 ms (enqueue 0.137712 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.08433 ms - Host latency: 6.23963 ms (enqueue 0.136017 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.10779 ms - Host latency: 6.26222 ms (enqueue 0.131744 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.10174 ms - Host latency: 6.25772 ms (enqueue 0.129745 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.10229 ms - Host latency: 6.25673 ms (enqueue 0.130112 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.05877 ms - Host latency: 6.21567 ms (enqueue 0.131021 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.05772 ms - Host latency: 6.21365 ms (enqueue 0.130246 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.05397 ms - Host latency: 6.21151 ms (enqueue 0.130225 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.06299 ms - Host latency: 6.22052 ms (enqueue 0.132465 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.07111 ms - Host latency: 6.22497 ms (enqueue 0.131329 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.07133 ms - Host latency: 6.22675 ms (enqueue 0.129462 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.06623 ms - Host latency: 6.22081 ms (enqueue 0.131 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.26959 ms - Host latency: 6.42663 ms (enqueue 0.13764 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.37431 ms - Host latency: 6.5326 ms (enqueue 0.132672 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.36928 ms - Host latency: 6.52493 ms (enqueue 0.131024 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.3593 ms - Host latency: 6.51505 ms (enqueue 0.129852 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.37102 ms - Host latency: 6.52395 ms (enqueue 0.130621 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.35966 ms - Host latency: 6.51382 ms (enqueue 0.131177 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.22831 ms - Host latency: 6.38276 ms (enqueue 0.129565 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.17354 ms - Host latency: 6.32644 ms (enqueue 0.131738 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.14226 ms - Host latency: 6.29685 ms (enqueue 0.131152 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.11937 ms - Host latency: 6.27281 ms (enqueue 0.134241 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.13983 ms - Host latency: 6.29264 ms (enqueue 0.131238 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.13169 ms - Host latency: 6.28965 ms (enqueue 0.134338 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.08741 ms - Host latency: 6.24911 ms (enqueue 0.132153 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.06295 ms - Host latency: 6.21763 ms (enqueue 0.133044 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.04734 ms - Host latency: 6.20264 ms (enqueue 0.13291 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.06766 ms - Host latency: 6.21909 ms (enqueue 0.131067 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.11897 ms - Host latency: 6.27424 ms (enqueue 0.13363 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.10134 ms - Host latency: 6.25532 ms (enqueue 0.131677 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.14211 ms - Host latency: 6.29409 ms (enqueue 0.131067 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.21559 ms - Host latency: 6.36751 ms (enqueue 0.134155 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.29246 ms - Host latency: 6.44735 ms (enqueue 0.131323 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.26545 ms - Host latency: 6.42137 ms (enqueue 0.147595 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.20891 ms - Host latency: 6.36399 ms (enqueue 0.132581 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.2944 ms - Host latency: 6.45192 ms (enqueue 0.144763 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.26052 ms - Host latency: 6.41209 ms (enqueue 0.131567 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.21931 ms - Host latency: 6.37217 ms (enqueue 0.132568 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.22532 ms - Host latency: 6.37827 ms (enqueue 0.131006 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.21033 ms - Host latency: 6.36494 ms (enqueue 0.133081 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.20144 ms - Host latency: 6.35227 ms (enqueue 0.131274 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.14919 ms - Host latency: 6.30151 ms (enqueue 0.132007 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.14453 ms - Host latency: 6.30022 ms (enqueue 0.1323 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.10708 ms - Host latency: 6.26897 ms (enqueue 0.131812 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.07817 ms - Host latency: 6.23848 ms (enqueue 0.131299 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.05774 ms - Host latency: 6.21199 ms (enqueue 0.13313 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.06157 ms - Host latency: 6.2167 ms (enqueue 0.142383 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.08933 ms - Host latency: 6.24309 ms (enqueue 0.134033 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.10693 ms - Host latency: 6.25981 ms (enqueue 0.132397 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.1374 ms - Host latency: 6.2916 ms (enqueue 0.131152 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.15286 ms - Host latency: 6.30977 ms (enqueue 0.132812 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.19802 ms - Host latency: 6.35017 ms (enqueue 0.1323 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.21313 ms - Host latency: 6.36614 ms (enqueue 0.132178 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.22612 ms - Host latency: 6.37825 ms (enqueue 0.131494 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.23765 ms - Host latency: 6.3908 ms (enqueue 0.132568 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.22866 ms - Host latency: 6.38032 ms (enqueue 0.131836 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.22837 ms - Host latency: 6.38333 ms (enqueue 0.130957 ms)\n",
      "[01/12/2026-14:33:57] [I] Average on 10 runs - GPU latency: 5.2054 ms - Host latency: 6.35615 ms (enqueue 0.131567 ms)\n",
      "[01/12/2026-14:33:57] [I] \n",
      "[01/12/2026-14:33:57] [I] === Performance summary ===\n",
      "[01/12/2026-14:33:57] [I] Throughput: 193.161 qps\n",
      "[01/12/2026-14:33:57] [I] Latency: min = 6.16724 ms, max = 6.55652 ms, mean = 6.32016 ms, median = 6.29663 ms, percentile(90%) = 6.47571 ms, percentile(95%) = 6.51904 ms, percentile(99%) = 6.54773 ms\n",
      "[01/12/2026-14:33:57] [I] Enqueue Time: min = 0.128174 ms, max = 0.265381 ms, mean = 0.132717 ms, median = 0.130859 ms, percentile(90%) = 0.137238 ms, percentile(95%) = 0.140381 ms, percentile(99%) = 0.151978 ms\n",
      "[01/12/2026-14:33:57] [I] H2D Latency: min = 0.910645 ms, max = 0.931763 ms, mean = 0.915623 ms, median = 0.914307 ms, percentile(90%) = 0.924194 ms, percentile(95%) = 0.927246 ms, percentile(99%) = 0.92981 ms\n",
      "[01/12/2026-14:33:57] [I] GPU Compute Time: min = 5.01526 ms, max = 5.39618 ms, mean = 5.16544 ms, median = 5.1438 ms, percentile(90%) = 5.31519 ms, percentile(95%) = 5.36414 ms, percentile(99%) = 5.39032 ms\n",
      "[01/12/2026-14:33:57] [I] D2H Latency: min = 0.232422 ms, max = 0.261475 ms, mean = 0.239098 ms, median = 0.238281 ms, percentile(90%) = 0.24469 ms, percentile(95%) = 0.246338 ms, percentile(99%) = 0.254578 ms\n",
      "[01/12/2026-14:33:57] [I] Total Host Walltime: 3.01303 s\n",
      "[01/12/2026-14:33:57] [I] Total GPU Compute Time: 3.00628 s\n",
      "[01/12/2026-14:33:57] [W] * GPU compute time is unstable, with coefficient of variance = 1.83297%.\n",
      "[01/12/2026-14:33:57] [W]   If not already in use, locking GPU clock frequency or adding --useSpinWait may improve the stability.\n",
      "[01/12/2026-14:33:57] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
      "[01/12/2026-14:33:57] [I] \n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v101200] [b36] # trtexec --onnx=/hwj/project/CompressAI-Science/examples/out_engines/subonnx/bmshj2018-factorized-q1/ga_fp16.onnx --saveEngine=/hwj/project/CompressAI-Science/examples/out_engines/engines/bmshj2018-factorized-q1/ga/fp16.engine --fp16\n",
      "[OK] FP16 Engine built: /hwj/project/CompressAI-Science/examples/out_engines/engines/bmshj2018-factorized-q1/ga/fp16.engine\n",
      "\n",
      "[Engine] Building gs engine (fp16): /hwj/project/CompressAI-Science/examples/out_engines/engines/bmshj2018-factorized-q1/gs/fp16.engine\n",
      "trtexec --onnx=/hwj/project/CompressAI-Science/examples/out_engines/subonnx/bmshj2018-factorized-q1/gs_fp16.onnx --saveEngine=/hwj/project/CompressAI-Science/examples/out_engines/engines/bmshj2018-factorized-q1/gs/fp16.engine --fp16\n",
      "&&&& RUNNING TensorRT.trtexec [TensorRT v101200] [b36] # trtexec --onnx=/hwj/project/CompressAI-Science/examples/out_engines/subonnx/bmshj2018-factorized-q1/gs_fp16.onnx --saveEngine=/hwj/project/CompressAI-Science/examples/out_engines/engines/bmshj2018-factorized-q1/gs/fp16.engine --fp16\n",
      "[01/12/2026-14:33:58] [I] === Model Options ===\n",
      "[01/12/2026-14:33:58] [I] Format: ONNX\n",
      "[01/12/2026-14:33:58] [I] Model: /hwj/project/CompressAI-Science/examples/out_engines/subonnx/bmshj2018-factorized-q1/gs_fp16.onnx\n",
      "[01/12/2026-14:33:58] [I] Output:\n",
      "[01/12/2026-14:33:58] [I] === Build Options ===\n",
      "[01/12/2026-14:33:58] [I] Memory Pools: workspace: default, dlaSRAM: default, dlaLocalDRAM: default, dlaGlobalDRAM: default, tacticSharedMem: default\n",
      "[01/12/2026-14:33:58] [I] avgTiming: 8\n",
      "[01/12/2026-14:33:58] [I] Precision: FP32+FP16\n",
      "[01/12/2026-14:33:58] [I] LayerPrecisions: \n",
      "[01/12/2026-14:33:58] [I] Layer Device Types: \n",
      "[01/12/2026-14:33:58] [I] Calibration: \n",
      "[01/12/2026-14:33:58] [I] Refit: Disabled\n",
      "[01/12/2026-14:33:58] [I] Strip weights: Disabled\n",
      "[01/12/2026-14:33:58] [I] Version Compatible: Disabled\n",
      "[01/12/2026-14:33:58] [I] ONNX Plugin InstanceNorm: Disabled\n",
      "[01/12/2026-14:33:58] [I] ONNX kENABLE_UINT8_AND_ASYMMETRIC_QUANTIZATION_DLA flag: Disabled\n",
      "[01/12/2026-14:33:58] [I] TensorRT runtime: full\n",
      "[01/12/2026-14:33:58] [I] Lean DLL Path: \n",
      "[01/12/2026-14:33:58] [I] Tempfile Controls: { in_memory: allow, temporary: allow }\n",
      "[01/12/2026-14:33:58] [I] Exclude Lean Runtime: Disabled\n",
      "[01/12/2026-14:33:58] [I] Sparsity: Disabled\n",
      "[01/12/2026-14:33:58] [I] Safe mode: Disabled\n",
      "[01/12/2026-14:33:58] [I] Build DLA standalone loadable: Disabled\n",
      "[01/12/2026-14:33:58] [I] Allow GPU fallback for DLA: Disabled\n",
      "[01/12/2026-14:33:58] [I] DirectIO mode: Disabled\n",
      "[01/12/2026-14:33:58] [I] Restricted mode: Disabled\n",
      "[01/12/2026-14:33:58] [I] Skip inference: Disabled\n",
      "[01/12/2026-14:33:58] [I] Save engine: /hwj/project/CompressAI-Science/examples/out_engines/engines/bmshj2018-factorized-q1/gs/fp16.engine\n",
      "[01/12/2026-14:33:58] [I] Load engine: \n",
      "[01/12/2026-14:33:58] [I] Profiling verbosity: 0\n",
      "[01/12/2026-14:33:58] [I] Tactic sources: Using default tactic sources\n",
      "[01/12/2026-14:33:58] [I] timingCacheMode: local\n",
      "[01/12/2026-14:33:58] [I] timingCacheFile: \n",
      "[01/12/2026-14:33:58] [I] Enable Compilation Cache: Enabled\n",
      "[01/12/2026-14:33:58] [I] Enable Monitor Memory: Disabled\n",
      "[01/12/2026-14:33:58] [I] errorOnTimingCacheMiss: Disabled\n",
      "[01/12/2026-14:33:58] [I] Preview Features: Use default preview flags.\n",
      "[01/12/2026-14:33:58] [I] MaxAuxStreams: -1\n",
      "[01/12/2026-14:33:58] [I] BuilderOptimizationLevel: -1\n",
      "[01/12/2026-14:33:58] [I] MaxTactics: -1\n",
      "[01/12/2026-14:33:58] [I] Calibration Profile Index: 0\n",
      "[01/12/2026-14:33:58] [I] Weight Streaming: Disabled\n",
      "[01/12/2026-14:33:58] [I] Runtime Platform: Same As Build\n",
      "[01/12/2026-14:33:58] [I] Debug Tensors: \n",
      "[01/12/2026-14:33:58] [I] Distributive Independence: Disabled\n",
      "[01/12/2026-14:33:58] [I] Mark Unfused Tensors As Debug Tensors: Disabled\n",
      "[01/12/2026-14:33:58] [I] Input(s)s format: fp32:CHW\n",
      "[01/12/2026-14:33:58] [I] Output(s)s format: fp32:CHW\n",
      "[01/12/2026-14:33:58] [I] Input build shapes: model\n",
      "[01/12/2026-14:33:58] [I] Input calibration shapes: model\n",
      "[01/12/2026-14:33:58] [I] === System Options ===\n",
      "[01/12/2026-14:33:58] [I] Device: 0\n",
      "[01/12/2026-14:33:58] [I] DLACore: \n",
      "[01/12/2026-14:33:58] [I] Plugins:\n",
      "[01/12/2026-14:33:58] [I] setPluginsToSerialize:\n",
      "[01/12/2026-14:33:58] [I] dynamicPlugins:\n",
      "[01/12/2026-14:33:58] [I] ignoreParsedPluginLibs: 0\n",
      "[01/12/2026-14:33:58] [I] \n",
      "[01/12/2026-14:33:58] [I] === Inference Options ===\n",
      "[01/12/2026-14:33:58] [I] Batch: Explicit\n",
      "[01/12/2026-14:33:58] [I] Input inference shapes: model\n",
      "[01/12/2026-14:33:58] [I] Iterations: 10\n",
      "[01/12/2026-14:33:58] [I] Duration: 3s (+ 200ms warm up)\n",
      "[01/12/2026-14:33:58] [I] Sleep time: 0ms\n",
      "[01/12/2026-14:33:58] [I] Idle time: 0ms\n",
      "[01/12/2026-14:33:58] [I] Inference Streams: 1\n",
      "[01/12/2026-14:33:58] [I] ExposeDMA: Disabled\n",
      "[01/12/2026-14:33:58] [I] Data transfers: Enabled\n",
      "[01/12/2026-14:33:58] [I] Spin-wait: Disabled\n",
      "[01/12/2026-14:33:58] [I] Multithreading: Disabled\n",
      "[01/12/2026-14:33:58] [I] CUDA Graph: Disabled\n",
      "[01/12/2026-14:33:58] [I] Separate profiling: Disabled\n",
      "[01/12/2026-14:33:58] [I] Time Deserialize: Disabled\n",
      "[01/12/2026-14:33:58] [I] Time Refit: Disabled\n",
      "[01/12/2026-14:33:58] [I] NVTX verbosity: 0\n",
      "[01/12/2026-14:33:58] [I] Persistent Cache Ratio: 0\n",
      "[01/12/2026-14:33:58] [I] Optimization Profile Index: 0\n",
      "[01/12/2026-14:33:58] [I] Weight Streaming Budget: 100.000000%\n",
      "[01/12/2026-14:33:58] [I] Inputs:\n",
      "[01/12/2026-14:33:58] [I] Debug Tensor Save Destinations:\n",
      "[01/12/2026-14:33:58] [I] Dump All Debug Tensor in Formats: \n",
      "[01/12/2026-14:33:58] [I] === Reporting Options ===\n",
      "[01/12/2026-14:33:58] [I] Verbose: Disabled\n",
      "[01/12/2026-14:33:58] [I] Averages: 10 inferences\n",
      "[01/12/2026-14:33:58] [I] Percentiles: 90,95,99\n",
      "[01/12/2026-14:33:58] [I] Dump refittable layers:Disabled\n",
      "[01/12/2026-14:33:58] [I] Dump output: Disabled\n",
      "[01/12/2026-14:33:58] [I] Profile: Disabled\n",
      "[01/12/2026-14:33:58] [I] Export timing to JSON file: \n",
      "[01/12/2026-14:33:58] [I] Export output to JSON file: \n",
      "[01/12/2026-14:33:58] [I] Export profile to JSON file: \n",
      "[01/12/2026-14:33:58] [I] \n",
      "[01/12/2026-14:33:58] [I] === Device Information ===\n",
      "[01/12/2026-14:33:58] [I] Available Devices: \n",
      "[01/12/2026-14:33:58] [I]   Device 0: \"NVIDIA H100 PCIe\" UUID: GPU-3e2fdfe4-208f-4000-27c1-3ecf36260172\n",
      "[01/12/2026-14:33:58] [I]   Device 1: \"NVIDIA H100 PCIe\" UUID: GPU-ad162803-098e-b980-5e1b-5623d5adc58b\n",
      "[01/12/2026-14:33:58] [I] Selected Device: NVIDIA H100 PCIe\n",
      "[01/12/2026-14:33:58] [I] Selected Device ID: 0\n",
      "[01/12/2026-14:33:58] [I] Selected Device UUID: GPU-3e2fdfe4-208f-4000-27c1-3ecf36260172\n",
      "[01/12/2026-14:33:58] [I] Compute Capability: 9.0\n",
      "[01/12/2026-14:33:58] [I] SMs: 114\n",
      "[01/12/2026-14:33:58] [I] Device Global Memory: 81079 MiB\n",
      "[01/12/2026-14:33:58] [I] Shared Memory per SM: 228 KiB\n",
      "[01/12/2026-14:33:58] [I] Memory Bus Width: 5120 bits (ECC enabled)\n",
      "[01/12/2026-14:33:58] [I] Application Compute Clock Rate: 1.755 GHz\n",
      "[01/12/2026-14:33:58] [I] Application Memory Clock Rate: 1.593 GHz\n",
      "[01/12/2026-14:33:58] [I] \n",
      "[01/12/2026-14:33:58] [I] Note: The application clock rates do not reflect the actual clock rates that the GPU is currently running at.\n",
      "[01/12/2026-14:33:58] [I] \n",
      "[01/12/2026-14:33:58] [I] TensorRT version: 10.12.0\n",
      "[01/12/2026-14:33:58] [I] Loading standard plugins\n",
      "[01/12/2026-14:33:58] [I] [TRT] [MemUsageChange] Init CUDA: CPU +2, GPU +0, now: CPU 44, GPU 5903 (MiB)\n",
      "[01/12/2026-14:34:00] [I] [TRT] [MemUsageChange] Init builder kernel library: CPU +1927, GPU +8, now: CPU 2173, GPU 5913 (MiB)\n",
      "[01/12/2026-14:34:00] [I] Start parsing network model.\n",
      "[01/12/2026-14:34:00] [I] [TRT] ----------------------------------------------------------------\n",
      "[01/12/2026-14:34:00] [I] [TRT] Input filename:   /hwj/project/CompressAI-Science/examples/out_engines/subonnx/bmshj2018-factorized-q1/gs_fp16.onnx\n",
      "[01/12/2026-14:34:00] [I] [TRT] ONNX IR version:  0.0.8\n",
      "[01/12/2026-14:34:00] [I] [TRT] Opset version:    17\n",
      "[01/12/2026-14:34:00] [I] [TRT] Producer name:    onnx.utils.extract_model\n",
      "[01/12/2026-14:34:00] [I] [TRT] Producer version: \n",
      "[01/12/2026-14:34:00] [I] [TRT] Domain:           \n",
      "[01/12/2026-14:34:00] [I] [TRT] Model version:    0\n",
      "[01/12/2026-14:34:00] [I] [TRT] Doc string:       \n",
      "[01/12/2026-14:34:00] [I] [TRT] ----------------------------------------------------------------\n",
      "[01/12/2026-14:34:00] [I] Finished parsing network model. Parse time: 0.00616891\n",
      "[01/12/2026-14:34:00] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[01/12/2026-14:36:37] [I] [TRT] Compiler backend is used during engine build.\n",
      "[01/12/2026-14:36:52] [I] [TRT] Detected 1 inputs and 1 output network tensors.\n",
      "[01/12/2026-14:36:53] [I] [TRT] Total Host Persistent Memory: 47168 bytes\n",
      "[01/12/2026-14:36:53] [I] [TRT] Total Device Persistent Memory: 91648 bytes\n",
      "[01/12/2026-14:36:53] [I] [TRT] Max Scratch Memory: 33280 bytes\n",
      "[01/12/2026-14:36:53] [I] [TRT] [BlockAssignment] Started assigning block shifts. This will take 53 steps to complete.\n",
      "[01/12/2026-14:36:53] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 2.3027ms to assign 21 blocks to 53 nodes requiring 1610751488 bytes.\n",
      "[01/12/2026-14:36:53] [I] [TRT] Total Activation Memory: 1610748416 bytes\n",
      "[01/12/2026-14:36:53] [I] [TRT] Total Weights Memory: 3022368 bytes\n",
      "[01/12/2026-14:36:53] [I] [TRT] Compiler backend is used during engine execution.\n",
      "[01/12/2026-14:36:53] [I] [TRT] Engine generation completed in 173.016 seconds.\n",
      "[01/12/2026-14:36:53] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 1 MiB, GPU 13832 MiB\n",
      "[01/12/2026-14:36:53] [I] Engine built in 173.066 sec.\n",
      "[01/12/2026-14:36:53] [I] Created engine with size: 3.82416 MiB\n",
      "[01/12/2026-14:36:53] [I] [TRT] Loaded engine size: 3 MiB\n",
      "[01/12/2026-14:36:53] [I] Engine deserialized in 0.0176248 sec.\n",
      "[01/12/2026-14:36:53] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +1537, now: CPU 0, GPU 1539 (MiB)\n",
      "[01/12/2026-14:36:53] [I] Setting persistentCacheLimit to 0 bytes.\n",
      "[01/12/2026-14:36:53] [I] Created execution context with device memory size: 1536.13 MiB\n",
      "[01/12/2026-14:36:53] [I] Using random values for input /entropy_bottleneck/Transpose_1_output_0\n",
      "[01/12/2026-14:36:54] [I] Input binding for /entropy_bottleneck/Transpose_1_output_0 with dimensions 512x192x8x8 is created.\n",
      "[01/12/2026-14:36:54] [I] Output binding for output with dimensions 512x3x128x128 is created.\n",
      "[01/12/2026-14:36:54] [I] Starting inference\n",
      "[01/12/2026-14:36:57] [I] Warmup completed 25 queries over 200 ms\n",
      "[01/12/2026-14:36:57] [I] Timing trace has 337 queries over 3.02461 s\n",
      "[01/12/2026-14:36:57] [I] \n",
      "[01/12/2026-14:36:57] [I] === Trace details ===\n",
      "[01/12/2026-14:36:57] [I] Trace averages of 10 runs:\n",
      "[01/12/2026-14:36:57] [I] Average on 10 runs - GPU latency: 8.7414 ms - Host latency: 9.92373 ms (enqueue 0.143596 ms)\n",
      "[01/12/2026-14:36:57] [I] Average on 10 runs - GPU latency: 8.669 ms - Host latency: 9.85349 ms (enqueue 0.168887 ms)\n",
      "[01/12/2026-14:36:57] [I] Average on 10 runs - GPU latency: 8.63588 ms - Host latency: 9.82007 ms (enqueue 0.163306 ms)\n",
      "[01/12/2026-14:36:57] [I] Average on 10 runs - GPU latency: 8.631 ms - Host latency: 9.81314 ms (enqueue 0.146597 ms)\n",
      "[01/12/2026-14:36:57] [I] Average on 10 runs - GPU latency: 8.6299 ms - Host latency: 9.81455 ms (enqueue 0.146252 ms)\n",
      "[01/12/2026-14:36:57] [I] Average on 10 runs - GPU latency: 8.6309 ms - Host latency: 9.81395 ms (enqueue 0.145276 ms)\n",
      "[01/12/2026-14:36:57] [I] Average on 10 runs - GPU latency: 8.902 ms - Host latency: 10.0985 ms (enqueue 0.145068 ms)\n",
      "[01/12/2026-14:36:57] [I] Average on 10 runs - GPU latency: 9.42831 ms - Host latency: 10.6471 ms (enqueue 0.165912 ms)\n",
      "[01/12/2026-14:36:57] [I] Average on 10 runs - GPU latency: 9.31968 ms - Host latency: 10.5265 ms (enqueue 0.146472 ms)\n",
      "[01/12/2026-14:36:57] [I] Average on 10 runs - GPU latency: 9.36805 ms - Host latency: 10.578 ms (enqueue 0.14953 ms)\n",
      "[01/12/2026-14:36:57] [I] Average on 10 runs - GPU latency: 9.23646 ms - Host latency: 10.43 ms (enqueue 0.150146 ms)\n",
      "[01/12/2026-14:36:57] [I] Average on 10 runs - GPU latency: 8.99625 ms - Host latency: 10.1914 ms (enqueue 0.153247 ms)\n",
      "[01/12/2026-14:36:57] [I] Average on 10 runs - GPU latency: 8.80002 ms - Host latency: 9.98307 ms (enqueue 0.145874 ms)\n",
      "[01/12/2026-14:36:57] [I] Average on 10 runs - GPU latency: 8.74136 ms - Host latency: 9.92356 ms (enqueue 0.150244 ms)\n",
      "[01/12/2026-14:36:57] [I] Average on 10 runs - GPU latency: 8.64839 ms - Host latency: 9.83226 ms (enqueue 0.154504 ms)\n",
      "[01/12/2026-14:36:57] [I] Average on 10 runs - GPU latency: 8.6278 ms - Host latency: 9.81238 ms (enqueue 0.144299 ms)\n",
      "[01/12/2026-14:36:57] [I] Average on 10 runs - GPU latency: 8.67764 ms - Host latency: 9.8626 ms (enqueue 0.144495 ms)\n",
      "[01/12/2026-14:36:57] [I] Average on 10 runs - GPU latency: 8.99119 ms - Host latency: 10.1854 ms (enqueue 0.145544 ms)\n",
      "[01/12/2026-14:36:57] [I] Average on 10 runs - GPU latency: 9.21886 ms - Host latency: 10.412 ms (enqueue 0.145642 ms)\n",
      "[01/12/2026-14:36:57] [I] Average on 10 runs - GPU latency: 9.26084 ms - Host latency: 10.4583 ms (enqueue 0.143762 ms)\n",
      "[01/12/2026-14:36:57] [I] Average on 10 runs - GPU latency: 9.30684 ms - Host latency: 10.5293 ms (enqueue 0.146191 ms)\n",
      "[01/12/2026-14:36:57] [I] Average on 10 runs - GPU latency: 9.24211 ms - Host latency: 10.4375 ms (enqueue 0.143872 ms)\n",
      "[01/12/2026-14:36:57] [I] Average on 10 runs - GPU latency: 8.97202 ms - Host latency: 10.1568 ms (enqueue 0.143286 ms)\n",
      "[01/12/2026-14:36:57] [I] Average on 10 runs - GPU latency: 8.96057 ms - Host latency: 10.1435 ms (enqueue 0.144897 ms)\n",
      "[01/12/2026-14:36:57] [I] Average on 10 runs - GPU latency: 8.77969 ms - Host latency: 9.96543 ms (enqueue 0.148779 ms)\n",
      "[01/12/2026-14:36:57] [I] Average on 10 runs - GPU latency: 8.70203 ms - Host latency: 9.88413 ms (enqueue 0.142188 ms)\n",
      "[01/12/2026-14:36:57] [I] Average on 10 runs - GPU latency: 8.6603 ms - Host latency: 9.84231 ms (enqueue 0.143848 ms)\n",
      "[01/12/2026-14:36:57] [I] Average on 10 runs - GPU latency: 8.672 ms - Host latency: 9.85542 ms (enqueue 0.1448 ms)\n",
      "[01/12/2026-14:36:57] [I] Average on 10 runs - GPU latency: 8.93201 ms - Host latency: 10.1206 ms (enqueue 0.144971 ms)\n",
      "[01/12/2026-14:36:57] [I] Average on 10 runs - GPU latency: 9.06177 ms - Host latency: 10.2524 ms (enqueue 0.143286 ms)\n",
      "[01/12/2026-14:36:57] [I] Average on 10 runs - GPU latency: 9.23103 ms - Host latency: 10.4213 ms (enqueue 0.143848 ms)\n",
      "[01/12/2026-14:36:57] [I] Average on 10 runs - GPU latency: 9.23889 ms - Host latency: 10.4271 ms (enqueue 0.144946 ms)\n",
      "[01/12/2026-14:36:57] [I] Average on 10 runs - GPU latency: 9.24465 ms - Host latency: 10.4375 ms (enqueue 0.144702 ms)\n",
      "[01/12/2026-14:36:57] [I] \n",
      "[01/12/2026-14:36:57] [I] === Performance summary ===\n",
      "[01/12/2026-14:36:57] [I] Throughput: 111.419 qps\n",
      "[01/12/2026-14:36:57] [I] Latency: min = 9.80383 ms, max = 10.7292 ms, mean = 10.1363 ms, median = 10.144 ms, percentile(90%) = 10.5251 ms, percentile(95%) = 10.6216 ms, percentile(99%) = 10.6578 ms\n",
      "[01/12/2026-14:36:57] [I] Enqueue Time: min = 0.138367 ms, max = 0.306366 ms, mean = 0.147741 ms, median = 0.143799 ms, percentile(90%) = 0.154236 ms, percentile(95%) = 0.167603 ms, percentile(99%) = 0.212616 ms\n",
      "[01/12/2026-14:36:57] [I] H2D Latency: min = 0.233643 ms, max = 0.256836 ms, mean = 0.237493 ms, median = 0.236206 ms, percentile(90%) = 0.241699 ms, percentile(95%) = 0.243591 ms, percentile(99%) = 0.250183 ms\n",
      "[01/12/2026-14:36:57] [I] GPU Compute Time: min = 8.62292 ms, max = 9.53979 ms, mean = 8.94567 ms, median = 8.96069 ms, percentile(90%) = 9.31299 ms, percentile(95%) = 9.40704 ms, percentile(99%) = 9.43311 ms\n",
      "[01/12/2026-14:36:57] [I] D2H Latency: min = 0.918213 ms, max = 0.993042 ms, mean = 0.95318 ms, median = 0.949951 ms, percentile(90%) = 0.970337 ms, percentile(95%) = 0.982056 ms, percentile(99%) = 0.990723 ms\n",
      "[01/12/2026-14:36:57] [I] Total Host Walltime: 3.02461 s\n",
      "[01/12/2026-14:36:57] [I] Total GPU Compute Time: 3.01469 s\n",
      "[01/12/2026-14:36:57] [W] * GPU compute time is unstable, with coefficient of variance = 3.08399%.\n",
      "[01/12/2026-14:36:57] [W]   If not already in use, locking GPU clock frequency or adding --useSpinWait may improve the stability.\n",
      "[01/12/2026-14:36:57] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
      "[01/12/2026-14:36:57] [I] \n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v101200] [b36] # trtexec --onnx=/hwj/project/CompressAI-Science/examples/out_engines/subonnx/bmshj2018-factorized-q1/gs_fp16.onnx --saveEngine=/hwj/project/CompressAI-Science/examples/out_engines/engines/bmshj2018-factorized-q1/gs/fp16.engine --fp16\n",
      "[OK] FP16 Engine built: /hwj/project/CompressAI-Science/examples/out_engines/engines/bmshj2018-factorized-q1/gs/fp16.engine\n",
      "\n",
      "Done. Engines saved under: /hwj/project/CompressAI-Science/examples/out_engines/engines/bmshj2018-factorized-q1\n"
     ]
    }
   ],
   "source": [
    "!python ./utils/build_engines.py \\\n",
    "--onnx_fp32 /hwj/data/model/onnx/bmshj2018-factorized-prior-1-f32.onnx \\\n",
    "--onnx_fp16 /hwj/data/model/onnx/bmshj2018-factorized-prior-1-f16.onnx \\\n",
    "--input_shape 512,3,128,128 \\\n",
    "--config ga-fp16,gs-fp16 \\\n",
    "--boundaries /hwj/project/CompressAI-Science/examples/config.json \\\n",
    "--calib_npy /hwj/project/aiz-accelerate/data/nyx-dark_matter_density.npy \\\n",
    "--out_dir /hwj/project/CompressAI-Science/examples/out_engines \\\n",
    "--model_tag bmshj2018-factorized-q1 \\\n",
    "--max_calib_samples 512 \\\n",
    "--prefer_cuda_ort\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0117b9cc",
   "metadata": {},
   "source": [
    "# Step 1 — Run Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43895536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_bytes': 100663296.0,\n",
       " 'enc_ms': 4.6011073112487795,\n",
       " 'dec_ms': 8.461580657958985,\n",
       " 'enc_GBps': 20.375529988357403,\n",
       " 'dec_GBps': 11.079490202793082,\n",
       " 'strings_bytes': 330396.0,\n",
       " 'state_bytes': 31.0,\n",
       " 'total_bytes': 330427.0,\n",
       " 'bpp_strings': 161.326171875,\n",
       " 'bpp_total': 161.34130859375,\n",
       " 'cr_strings': 304.67468129154105,\n",
       " 'cr_total': 304.64609732255536,\n",
       " 'rmse': 0.10539152473211288,\n",
       " 'nrmse': 0.10580645501613617,\n",
       " 'maxe': 0.9387968182563782,\n",
       " 'psnr': 13.489158630371094}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from compressai.zoo import bmshj2018_factorized\n",
    "from compressai.runtime import build_runtime\n",
    "from compressai.runtime.config import RuntimeConfig\n",
    "from compressai.runtime.codecs import GpuPackedEntropyCodec\n",
    "from compressai.runtime.utils.benchmark import run_e2e\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "# 1) load net\n",
    "net = bmshj2018_factorized(quality=1, pretrained=False).to(device).eval()\n",
    "state = torch.load(\"/hwj/data/model/bmshj2018-factorized-prior-1.pth\", map_location=device)\n",
    "net.load_state_dict(state)\n",
    "\n",
    "# 2) codec (in runtime)\n",
    "codec = GpuPackedEntropyCodec(net.entropy_bottleneck, P=12)\n",
    "\n",
    "# 3) runtime (TRT, dtype auto-infer)\n",
    "cfg = RuntimeConfig(\n",
    "    mode=\"trt\",\n",
    "    ga_input_dtype=torch.float32,\n",
    "    gs_input_dtype=torch.float16,\n",
    "    codec_input_dtype=torch.float32,\n",
    "    trt_engines={\n",
    "        \"ga\": \"/hwj/project/CompressAI-Science/examples/out_engines/engines/bmshj2018-factorized-q1/ga/fp8.engine\",\n",
    "        \"gs\": \"/hwj/project/CompressAI-Science/examples/out_engines/engines/bmshj2018-factorized-q1/gs/fp16.engine\",\n",
    "    },\n",
    ")\n",
    "engine = build_runtime(net, codec, cfg)\n",
    "\n",
    "# 4) data\n",
    "arr = np.load(\"/hwj/project/aiz-accelerate/data/nyx-dark_matter_density.npy\")\n",
    "x = torch.from_numpy(arr).float().to(device)\n",
    "\n",
    "# 5) benchmark (auto stream)\n",
    "stats, x_hat, x = run_e2e(engine, codec, x, warmup=3, iters=5)\n",
    "stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91f41dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0'),\n",
       " tensor([0.0471, 0.0235, 0.1216, 0.0706, 0.0902, 0.0941, 0.0627, 0.1882, 0.0784,\n",
       "         0.1882], device='cuda:0'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_hat[0,0,0,:10], x[0,0,0,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c95b2ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compressai-s",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
